<!DOCTYPE html>
<html lang="en">

<head>
    <title>Neural Networks - 6 December, 2016</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="keywords" content="machine learning">
    <meta name="author" content="Eugene Pyl">
    <link rel="icon" href="./../favicon.ico" />
    <link rel="stylesheet" href="./../prism.css">
    
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script>
    <script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5906f3ca75e4e1001109c19a&product=inline-share-buttons"></script>
    <style>
        img {
            max-width: 100%;
            height: auto;
            width: auto\9;
            /* ie8 */
        }
        
        body {
            font-family: 'Verdana', sans-serif;
            line-height: 1.3rem;
            box-sizing: border-box;
            min-width: 200px;
            max-width: 980px;
            margin: 0 auto;
            padding: 1rem;
            background-image: url(./../bg01.png)
        }
        
        h2 {
            font-size: 1.3rem;
        }
        
        code.has-jax {
            font: inherit;
            font-size: 100%;
            background: inherit;
            border: inherit;
        }
        
        .markdown-body pre {
            border: 1px solid black;
        }
        
        div.shareit {
            margin-top: 0.5rem;
        }
        
        article>h2 {
            margin-top: 0;
        }
    </style>
</head>

<body>
    <article class="markdown-body">
        <h2 id="neural-networks-6-december-2016">Neural Networks - 6 December, 2016</h2>
<p>Tags: machine learning</p>
<p>Neural networks are limited imitations of how our own brains work. They&#39;ve had a big recent resurgence because of advances in computer hardware.</p>
<p>In neural networks, we use the same logistic function as in classification: <code>\(\frac{1}{1 + e^{-\theta^Tx}}\)</code>. In neural networks however we sometimes call it a sigmoid (logistic) activation function.</p>
<p>Visually, a simplistic representation looks like:</p>
<p><code>\[\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline x_3\end{bmatrix}\rightarrow\begin{bmatrix}a_1^{(2)} \newline a_2^{(2)} \newline a_3^{(2)} \newline \end{bmatrix}\rightarrow h_\theta(x)\]</code></p>
<p>The values for each of the &quot;activation&quot; nodes is obtained as follows:</p>
<p><code>\[\begin{align*}
a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline
a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline
a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline
h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline
\end{align*}\]</code></p>
<p>Each layer gets its own matrix of weights, <code>\(\Theta^{(j)}\)</code>.</p>
<p>The dimensions of these matrices of weights is determined as follows:</p>
<p><code>\[\text{If network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $\Theta^{(j)}$ will be of dimension $s_{j+1} \times (s_j + 1)$.}\]</code></p>
<h3 id="multiclass-classification">Multiclass Classification</h3>
<p><code>\[\begin{align*}\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline\cdots \newline x_n\end{bmatrix} \rightarrow\begin{bmatrix}a_0^{(2)} \newline a_1^{(2)} \newline a_2^{(2)} \newline\cdots\end{bmatrix} \rightarrow\begin{bmatrix}a_0^{(3)} \newline a_1^{(3)} \newline a_2^{(3)} \newline\cdots\end{bmatrix} \rightarrow \cdots \rightarrow\begin{bmatrix}h_\Theta(x)_1 \newline h_\Theta(x)_2 \newline h_\Theta(x)_3 \newline h_\Theta(x)_4 \newline\end{bmatrix} \rightarrow\end{align*}\]</code></p>
<p>We can define our set of resulting classes as y:</p>
<p><img src="./../images/multi_neur_res.png" alt="Neur_multi_res"></p>
<p>Our final value of our hypothesis for a set of inputs will be one of the elements in y.</p>

    </article>
    <div class="sharethis-inline-share-buttons"></div>
    <div id="disqus_thread"></div>
    <script>
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document,
                s = d.createElement('script');
            s.src = '//eapyl-github-io.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] } }); MathJax.Hub.Queue(function() { // Fix <code> tags after MathJax finishes running. This is a
        // hack to overcome a shortcoming of Markdown. Discussion at
        // https://github.com/mojombo/jekyll/issues/199
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
<script>
    (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-87583712-1', 'auto');
    ga('send', 'pageview');

</script>
<script src="./../prism.js"></script>
</body>

</html>