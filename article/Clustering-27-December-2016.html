<!DOCTYPE html>
<html lang="en">

<head>
    <title>Clustering - 27 December, 2016</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="keywords" content="machine learning">
    <meta name="author" content="Eugene Pyl">
    <link rel="icon" href="./../favicon.ico" />
    <link rel="stylesheet" href="./../prism.css">
    
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script>
    <script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5906f3ca75e4e1001109c19a&product=inline-share-buttons"></script>
    <style>
        img {
            max-width: 100%;
            height: auto;
            width: auto\9;
            /* ie8 */
        }
        
        body {
            font-family: 'Verdana', sans-serif;
            line-height: 1.3rem;
            box-sizing: border-box;
            min-width: 200px;
            max-width: 980px;
            margin: 0 auto;
            background-image: url(./../bg02.png);
            background-color: #e8e8e8;
        }

        body > div {
            background-image: url(./../bg02.png);
            background-color: #fff;
            padding: 1rem;
            box-shadow: 0 0 0.25em 0em rgba(0, 0, 0, 0.25);
        }
        
        h2 {
            font-size: 1.3rem;
        }
        
        code.has-jax {
            font: inherit;
            font-size: 100%;
            background: inherit;
            border: inherit;
        }
        
        .markdown-body pre {
            border: 1px solid black;
        }
        
        div.shareit {
            margin-top: 0.5rem;
        }
        
        article>h2 {
            margin-top: 0;
        }
    </style>
</head>

<body>
    <div>
    <article class="markdown-body">
        <h2 id="clustering-27-december-2016">Clustering - 27 December, 2016</h2>
<p>Tags: machine learning</p>
<p>Unsupervised learning is contrasted from supervised learning because it uses an unlabeled training set rather than a labeled one.</p>
<h3 id="k-means-algorithm">K-Means Algorithm</h3>
<ol>
<li>Randomly initialize two points in the dataset called the cluster centroids.</li>
<li>Cluster assignment: assign all examples into one of two groups based on which cluster centroid the example is closest to.</li>
<li>Move centroid: compute the averages for all the points inside each of the two cluster centroid groups, then move the cluster centroid points to those averages.</li>
<li>Re-run (2) and (3) until we have found our clusters.</li>
</ol>
<pre><code class="language-none">Randomly initialize K cluster centroids mu(1), mu(2), ..., mu(K)
Repeat:
   for i = 1 to m:
      c(i):= index (from 1 to K) of cluster centroid closest to x(i)
   for k = 1 to K:
      mu(k):= average (mean) of points assigned to cluster k
</code></pre>
<p>The first for-loop is the &#39;Cluster Assignment&#39; step. We make a vector c where c(i) represents the centroid assigned to example x(i).</p>
<p><code>\[c^{(i)} = argmin_k\ ||x^{(i)} - \mu_k||^2\]</code></p>
<p><code>\[||x^{(i)} - \mu_k|| = ||\quad\sqrt{(x_1^i - \mu_{1(k)})^2 + (x_2^i - \mu_{2(k)})^2 + (x_3^i - \mu_{3(k)})^2 + ...}\quad||\]</code></p>
<p>The second for-loop is the &#39;Move Centroid&#39; step where we move each centroid to the average of its group.</p>
<p><code>\[\mu_k = \dfrac{1}{n}[x^{(k_1)} + x^{(k_2)} + \dots + x^{(k_n)}] \in \mathbb{R}^n\]</code></p>
<p>After a number of iterations the algorithm will converge, where new iterations do not affect the clusters.</p>
<h3 id="optimization-objective">Optimization Objective</h3>
<p>Using these variables we can define our cost function:</p>
<p><code>\[J(c^{(i)},\dots,c^{(m)},\mu_1,\dots,\mu_K) = \dfrac{1}{m}\sum_{i=1}^m ||x^{(i)} - \mu_{c^{(i)}}||^2\]</code></p>
<p>With k-means, it is not possible for the cost function to sometimes increase. It should always descend.</p>
<h3 id="random-initialization">Random Initialization</h3>
<ol>
<li>Have K&lt;m. That is, make sure the number of your clusters is less than the number of your training examples.</li>
<li>Randomly pick K training examples. (Not mentioned in the lecture, but also be sure the selected examples are unique).</li>
<li>Set μ1,…,μK equal to these K examples.</li>
</ol>
<h3 id="choosing-the-number-of-clusters">Choosing the Number of Clusters</h3>
<p>Choosing K can be quite arbitrary and ambiguous.</p>
<p>A way to choose K is to observe how well k-means performs on a downstream purpose. In other words, you choose K that proves to be most useful for some goal you&#39;re trying to achieve from using these clusters.</p>
<h3 id="dimensionality-reduction">Dimensionality Reduction</h3>
<p>Motivation I: Data Compression</p>
<ol>
<li>We may want to reduce the dimension of our features if we have a lot of redundant data.</li>
<li>To do this, we find two highly correlated features, plot them, and make a new line that seems to describe both features accurately. We place all the new features on this single line.</li>
</ol>
<p>Doing dimensionality reduction will reduce the total data we have to store in computer memory and will speed up our learning algorithm.</p>
<p>Motivation II: Visualization</p>
<p>It is not easy to visualize data that is more than three dimensions. We can reduce the dimensions of our data to 3 or less in order to plot it.</p>
<h3 id="principal-component-analysis-problem-formulation">Principal Component Analysis Problem Formulation</h3>
<p>The most popular dimensionality reduction algorithm is Principal Component Analysis (PCA)</p>
<p>Problem formulation</p>
<p>Given two features, x1 and x2, we want to find a single line that effectively describes both features at once. We then map our old features onto this new line to get a new single feature.</p>
<p>The goal of PCA is to reduce the average of all the distances of every feature to the projection line. This is the projection error.</p>
<p><strong>PCA is not linear regression</strong></p>
<ul>
<li>In linear regression, we are minimizing the squared error from every point to our predictor line. These are vertical distances.</li>
<li>In PCA, we are minimizing the shortest distance, or shortest orthogonal distances, to our data points.</li>
</ul>
<h3 id="principal-component-analysis-algorithm">Principal Component Analysis Algorithm</h3>
<ul>
<li>Given training set: x(1),x(2),…,x(m)</li>
<li>Preprocess (feature scaling/mean normalization):</li>
</ul>
<p><code>\[\mu_j = \dfrac{1}{m}\sum^m_{i=1}x_j^{(i)}\]</code></p>
<ul>
<li>Replace each <code>\(x_j^{(i)}\)</code> with <code>\(x_j^{(i)} - \mu_j\)</code></li>
<li><p>If different features on different scales (e.g., x1 = size of house, x2 = number of bedrooms), scale features to have comparable range of values.</p>
</li>
<li><p>Compute &quot;covariance matrix&quot;</p>
</li>
</ul>
<p><code>\[\Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T\]</code></p>
<ol start="2">
<li>Compute &quot;eigenvectors&quot; of covariance matrix Σ</li>
</ol>
<pre><code class="language-none">[U,S,V] = svd(Sigma);
</code></pre>
<ol start="3">
<li>Take the first k columns of the U matrix and compute z</li>
</ol>
<p>Summarize:</p>
<pre><code class="language-none">Sigma = (1/m) * X&#39; * X; % compute the covariance matrix
[U,S,V] = svd(Sigma);   % compute our projected directions
Ureduce = U(:,1:k);     % take the first k directions
Z = X * Ureduce;        % compute the projected data points
</code></pre>
<h3 id="reconstruction-from-compressed-representation">Reconstruction from Compressed Representation</h3>
<p><code>\[x_{approx}^{(1)} = U_{reduce} \cdot z^{(1)}\]</code></p>
<h3 id="choosing-the-number-of-principal-components">Choosing the Number of Principal Components</h3>
<p>Algorithm for choosing k</p>
<ol>
<li>Try PCA with k=1,2,…</li>
<li>Compute <code>\(U_{reduce}, z, x\)</code></li>
<li>Check the formula given above that 99% of the variance is retained. If not, go to step one and increase k.</li>
</ol>
<h3 id="advice-for-applying-pca">Advice for Applying PCA</h3>
<ul>
<li>Compressions</li>
<li>Reduce space of data</li>
<li>Speed up algorithm</li>
<li>Visualization of data</li>
</ul>
<p><strong>Bad use of PCA</strong>: trying to prevent overfitting. </p>
<p>Don&#39;t assume you need to do PCA. <strong>Try your full machine learning algorithm without PCA first</strong>. Then use PCA if you find that you need it.</p>
<p>More info:
<a href="https://www.coursera.org/learn/machine-learning">https://www.coursera.org/learn/machine-learning</a></p>

    </article>
    <div class="sharethis-inline-share-buttons"></div>
    <div id="disqus_thread"></div>
    <script>
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document,
                s = d.createElement('script');
            s.src = '//eapyl-github-io.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] } }); MathJax.Hub.Queue(function() { // Fix <code> tags after MathJax finishes running. This is a
        // hack to overcome a shortcoming of Markdown. Discussion at
        // https://github.com/mojombo/jekyll/issues/199
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
<script>
    (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-87583712-1', 'auto');
    ga('send', 'pageview');

</script>
<script src="./../prism.js"></script>
<script>
    
</script>
</div>
</body>

</html>