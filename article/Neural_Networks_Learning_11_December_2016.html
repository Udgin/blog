<!DOCTYPE html>
<html>

<head>
    <title>Neural Networks: Learning - 11 December, 2016</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="keywords" content="machine learning">
    <meta name="author" content="Eugene Pyl">
    <link rel="icon" href="./../favicon.ico" />
    <link rel="stylesheet" href="./../markdown.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/styles/default.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script>
<script type="text/javascript">var switchTo5x=true;</script>
<script type="text/javascript" id="st_insights_js" src="https://ws.sharethis.com/button/buttons.js?publisher=651fb92c-bcb0-4320-b085-96e63a88cc7a"></script>
<script type="text/javascript">stLight.options({publisher: "651fb92c-bcb0-4320-b085-96e63a88cc7a", doNotHash: false, doNotCopy: false, hashAddressBar: false});</script>
    <style>
        body {
            box-sizing: border-box;
            min-width: 200px;
            max-width: 980px;
            margin: 0 auto;
            padding: 45px;
            background-color: #fcfcfc;
        }
        
        code.has-jax {
            font: inherit;
            font-size: 100%;
            background: inherit;
            border: inherit;
        }
        
        .night body {
            background: #fffedd;
        }

        .markdown-body pre{
            border: 1px solid black;
        }

        div.shareit{
            margin-top: 0.5rem;
        }

    </style>
</head>

<body>
    <article class="markdown-body">
        <h2 id="neural-networks-learning-11-december-2016">Neural Networks: Learning - 11 December, 2016</h2>
<p>Tags: machine learning</p>
<h3 id="cost-function">Cost Function</h3>
<p>a) L= total number of layers in the network</p>
<p>b) <code>\(s_l\)</code> = number of units (not counting bias unit) in layer l</p>
<p>c) K= number of output units/classes</p>
<p><code>\[\begin{gather*}\large J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\end{gather*}\]</code></p>
<h3 id="backpropagation-algorithm">Backpropagation Algorithm</h3>
<p>&quot;Backpropagation&quot; is neural-network terminology for minimizing our cost function, just like what we were doing with gradient descent in logistic and linear regression.</p>
<p>In back propagation we&#39;re going to compute for every node:</p>
<p><code>\(\delta_j^{(l)}\)</code> - = &quot;error&quot; of node j in layer l</p>
<p><code>\(a_j^{(l)}\)</code> is activation node j in layer l.</p>
<p>For the last layer, we can compute the vector of delta values with:</p>
<p><code>\[\delta^{(L)} = a^{(L)} - y\]</code></p>
<p>To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left:</p>
<p><code>\[\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ g&#39;(z^{(l)})\]</code></p>
<p><code>\[g&#39;(z^{(l)}) = a^{(l)}\ .*\ (1 - a^{(l)})\]</code></p>
<h3 id="backpropagation-algorithm">Backpropagation algorithm</h3>
<p>Given training set <code>\(\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace\)</code>;</p>
<ul>
<li>Set <code>\(\Delta^{(l)}_{i,j}\)</code>= 0 for all (l,i,j)</li>
</ul>
<p>For training example t =1 to m:</p>
<ul>
<li>Set <code>\(a^{(1)} := x^{(t)}\)</code></li>
<li>Perform forward propagation to compute <code>\(a^{(l)}\)</code> for l=2,3,…,L</li>
<li>Using <code>\(y^{(t)}\)</code>, compute <code>\(\delta^{(L)} = a^{(L)} - y^{(t)}\)</code></li>
<li>Compute <code>\(\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}\)</code> using <code>\(\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})\)</code></li>
<li><code>\(\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}\)</code> or with vectorization, <code>\(\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T\)</code></li>
<li><code>\(D^{(l)}_{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}_{i,j} + \lambda\Theta^{(l)}_{i,j}\right)\)</code> If j≠0 NOTE: Typo in lecture slide omits outside parentheses. This version * is correct.</li>
<li><code>\(D^{(l)}_{i,j} := \dfrac{1}{m}\Delta^{(l)}_{i,j}\)</code> If j=0</li>
</ul>
<h3 id="gradient-checking">Gradient Checking</h3>
<p>Gradient checking will assure that our backpropagation works as intended.</p>
<p><code>\[\dfrac{\partial}{\partial\Theta}J(\Theta) \approx \dfrac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}\]</code></p>
<p>Once you&#39;ve verified once that your backpropagation algorithm is correct, then you don&#39;t need to compute gradApprox again. The code to compute gradApprox is very slow.</p>
<h3 id="random-initialization">Random Initialization</h3>
<p>Initializing all theta weights to zero does not work with neural networks. When we backpropagate, all nodes will update to the same value repeatedly.</p>
<h3 id="summary">Summary</h3>
<p>First, pick a network architecture; choose the layout of your neural network, including how many hidden units in each layer and how many layers total.</p>
<ul>
<li>Number of input units = dimension of features <code>\(x^{(i)}\)</code></li>
<li>Number of output units = number of classes</li>
<li>Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units)</li>
<li>Defaults: 1 hidden layer. If more than 1 hidden layer, then the same number of units in every hidden * layer.</li>
</ul>
<p><strong>Training a Neural Network</strong></p>
<ol>
<li>Randomly initialize the weights</li>
<li>Implement forward propagation to get <code>\(h_\theta(x^{(i)})\)</code></li>
<li>Implement the cost function</li>
<li>Implement backpropagation to compute partial derivatives</li>
<li>Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.</li>
<li>Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.</li>
</ol>
<p>When we perform forward and back propagation, we loop on every training example:</p>
<pre><code>for i = 1:m,
   Perform forward propagation and backpropagation using example (x(i),y(i))
   (Get activations a(l) and delta terms d(l) for l = 2,...,L
</code></pre>
    </article>
    <div class="shareit"><span class='st_sharethis' displayText='ShareThis'></span>
<span class='st_facebook' displayText='Facebook'></span>
<span class='st_twitter' displayText='Tweet'></span>
<span class='st_linkedin' displayText='LinkedIn'></span>
<span class='st_pinterest' displayText='Pinterest'></span>
<span class='st_email' displayText='Email'></span></div>
    <div id="disqus_thread"></div>
    <script>
        (function () { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = '//eapyl-github-io.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
<script>
    hljs.initHighlighting();
</script>
<script>
        function timeCheck() {
            var currentTime = new Date().getHours();
            if (currentTime > 18 || currentTime < 6) {
                document.documentElement.classList.add('night');
            } else {
                document.documentElement.classList.remove('night');
            }
        }
        timeCheck();
        setInterval(timeCheck(), 1000 * 60 * 60);
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        }
    });
    MathJax.Hub.Queue(function() {
        // Fix <code> tags after MathJax finishes running. This is a
        // hack to overcome a shortcoming of Markdown. Discussion at
        // https://github.com/mojombo/jekyll/issues/199
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
<script>
    (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-87583712-1', 'auto');
    ga('send', 'pageview');

</script>
</body>

</html>