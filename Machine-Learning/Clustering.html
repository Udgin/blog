<html>

<head>
    <title>Clustering</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">
    <link rel="stylesheet" href="..\prism.css"></link>
    <style>
        .columns .column {
            padding: .4rem;
        }
        #sidebar-id {
            padding: .4rem;
        }
        code {
            padding: 0;
            overflow-x: scroll;
        }
        img {
            max-width: 100%;
        }
        .off-canvas .off-canvas-sidebar
        {
            background: none;
        }
    </style>
</head>

<body>
    <div class="off-canvas off-canvas-sidebar-show">
        <a class="off-canvas-toggle btn btn-primary btn-action" href="#sidebar-id">
            <i class="icon icon-menu"></i>
        </a>
        <div id="sidebar-id" class="off-canvas-sidebar">
            <ul class="menu">
                <li class="menu-item">
                    <div class="tile tile-centered">
                        <div class="tile-icon"><img class="avatar" src="https://media.licdn.com/dms/image/C4E03AQEtgQbQGiCpUQ/profile-displayphoto-shrink_200_200/0?e=1547683200&v=beta&t=YKT1NVMDvlzVocWQtSP2Y6Z4Eoy-fqPLncAIk45nF2U"
                                alt="Avatar"></div>
                        <div class="tile-content">Yauhen Pyl</div>
                    </div>
                </li>
                <li class="divider" data-content="LINKS">
                </li>
                <li class="menu-item">
                    <a href="https://plus.google.com/+UdginPyl">Google+</a>
                </li>
                <li class="menu-item">
                    <a href="https://www.facebook.com/yauhen.pyl">Facebook</a>
                </li>
                <li class="menu-item">
                    <a href="https://pl.linkedin.com/in/eugenepyl">LinkedIn</a>
                </li>
                <li class="menu-item">
                    <a href="https://github.com/eapyl">GitHub</a>
                </li>
                <li class="menu-item">
                    <a href="mailto:gromkaktus@gmail.com">Email</a>
                </li>
            </ul>
        </div>
        <a class="off-canvas-overlay" href="#close"></a>
        <div class="off-canvas-content">
            <div class="container grid-lg">
                <h1 id="clustering">Clustering</h1>
<p>Unsupervised learning is contrasted from supervised learning because it uses an unlabeled training set rather than a labeled one.</p>
<h3 id="k-means-algorithm">K-Means Algorithm</h3>
<ol>
<li>Randomly initialize two points in the dataset called the cluster centroids.</li>
<li>Cluster assignment: assign all examples into one of two groups based on which cluster centroid the example is closest to.</li>
<li>Move centroid: compute the averages for all the points inside each of the two cluster centroid groups, then move the cluster centroid points to those averages.</li>
<li>Re-run (2) and (3) until we have found our clusters.</li>
</ol>
<pre><code class="language-none">Randomly initialize K cluster centroids mu(1), mu(2), ..., mu(K)
Repeat:
   for i = 1 to m:
      c(i):= index (from 1 to K) of cluster centroid closest to x(i)
   for k = 1 to K:
      mu(k):= average (mean) of points assigned to cluster k
</code></pre>
<p>The first for-loop is the 'Cluster Assignment' step. We make a vector c where c(i) represents the centroid assigned to example x(i).</p>
<p><span class="math">\[c^{(i)} = argmin_k\ ||x^{(i)} - \mu_k||^2\]</span></p>
<p><span class="math">\[||x^{(i)} - \mu_k|| = ||\quad\sqrt{(x_1^i - \mu_{1(k)})^2 + (x_2^i - \mu_{2(k)})^2 + (x_3^i - \mu_{3(k)})^2 + ...}\quad||\]</span></p>
<p>The second for-loop is the 'Move Centroid' step where we move each centroid to the average of its group.</p>
<p><span class="math">\[\mu_k = \dfrac{1}{n}[x^{(k_1)} + x^{(k_2)} + \dots + x^{(k_n)}] \in \mathbb{R}^n\]</span></p>
<p>After a number of iterations the algorithm will converge, where new iterations do not affect the clusters.</p>
<h3 id="optimization-objective">Optimization Objective</h3>
<p>Using these variables we can define our cost function:</p>
<p><span class="math">\[J(c^{(i)},\dots,c^{(m)},\mu_1,\dots,\mu_K) = \dfrac{1}{m}\sum_{i=1}^m ||x^{(i)} - \mu_{c^{(i)}}||^2\]</span></p>
<p>With k-means, it is not possible for the cost function to sometimes increase. It should always descend.</p>
<h3 id="random-initialization">Random Initialization</h3>
<ol>
<li>Have K&lt;m. That is, make sure the number of your clusters is less than the number of your training examples.</li>
<li>Randomly pick K training examples. (Not mentioned in the lecture, but also be sure the selected examples are unique).</li>
<li>Set μ1,…,μK equal to these K examples.</li>
</ol>
<h3 id="choosing-the-number-of-clusters">Choosing the Number of Clusters</h3>
<p>Choosing K can be quite arbitrary and ambiguous.</p>
<p>A way to choose K is to observe how well k-means performs on a downstream purpose. In other words, you choose K that proves to be most useful for some goal you're trying to achieve from using these clusters.</p>
<h3 id="dimensionality-reduction">Dimensionality Reduction</h3>
<p>Motivation I: Data Compression</p>
<ol>
<li>We may want to reduce the dimension of our features if we have a lot of redundant data.</li>
<li>To do this, we find two highly correlated features, plot them, and make a new line that seems to describe both features accurately. We place all the new features on this single line.</li>
</ol>
<p>Doing dimensionality reduction will reduce the total data we have to store in computer memory and will speed up our learning algorithm.</p>
<p>Motivation II: Visualization</p>
<p>It is not easy to visualize data that is more than three dimensions. We can reduce the dimensions of our data to 3 or less in order to plot it.</p>
<h3 id="principal-component-analysis-problem-formulation">Principal Component Analysis Problem Formulation</h3>
<p>The most popular dimensionality reduction algorithm is Principal Component Analysis (PCA)</p>
<p>Problem formulation</p>
<p>Given two features, x1 and x2, we want to find a single line that effectively describes both features at once. We then map our old features onto this new line to get a new single feature.</p>
<p>The goal of PCA is to reduce the average of all the distances of every feature to the projection line. This is the projection error.</p>
<p><strong>PCA is not linear regression</strong></p>
<ul>
<li>In linear regression, we are minimizing the squared error from every point to our predictor line. These are vertical distances.</li>
<li>In PCA, we are minimizing the shortest distance, or shortest orthogonal distances, to our data points.</li>
</ul>
<h3 id="principal-component-analysis-algorithm">Principal Component Analysis Algorithm</h3>
<ul>
<li>Given training set: x(1),x(2),…,x(m)</li>
<li>Preprocess (feature scaling/mean normalization):</li>
</ul>
<p><span class="math">\[\mu_j = \dfrac{1}{m}\sum^m_{i=1}x_j^{(i)}\]</span></p>
<ul>
<li>Replace each <span class="math">\(x_j^{(i)}\)</span> with <span class="math">\(x_j^{(i)} - \mu_j\)</span></li>
<li>If different features on different scales (e.g., x1 = size of house, x2 = number of bedrooms), scale features to have comparable range of values.</li>
</ul>
<ol>
<li>Compute &quot;covariance matrix&quot;</li>
</ol>
<p><span class="math">\[Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T\]</span></p>
<ol start="2">
<li>Compute &quot;eigenvectors&quot; of covariance matrix Σ</li>
</ol>
<pre><code class="language-none">[U,S,V] = svd(Sigma);
</code></pre>
<ol start="3">
<li>Take the first k columns of the U matrix and compute z</li>
</ol>
<p>Summarize:</p>
<pre><code class="language-none">Sigma = (1/m) * X' * X; % compute the covariance matrix
[U,S,V] = svd(Sigma);   % compute our projected directions
Ureduce = U(:,1:k);     % take the first k directions
Z = X * Ureduce;        % compute the projected data points
</code></pre>
<h3 id="reconstruction-from-compressed-representation">Reconstruction from Compressed Representation</h3>
<p><span class="math">\[x_{approx}^{(1)} = U_{reduce} \cdot z^{(1)}\]</span></p>
<h3 id="choosing-the-number-of-principal-components">Choosing the Number of Principal Components</h3>
<p>Algorithm for choosing k</p>
<ol>
<li>Try PCA with k=1,2,…</li>
<li>Compute <span class="math">\(U_{reduce}, z, x\)</span></li>
<li>Check the formula given above that 99% of the variance is retained. If not, go to step one and increase k.</li>
</ol>
<h3 id="advice-for-applying-pca">Advice for Applying PCA</h3>
<ul>
<li>Compressions</li>
<li>Reduce space of data</li>
<li>Speed up algorithm</li>
<li>Visualization of data</li>
</ul>
<p><strong>Bad use of PCA</strong>: trying to prevent overfitting.</p>
<p>Don't assume you need to do PCA. <strong>Try your full machine learning algorithm without PCA first</strong>. Then use PCA if you find that you need it.</p>
<p>More info:
<a href="https://www.coursera.org/learn/machine-learning">https://www.coursera.org/learn/machine-learning</a></p>

                <div class="sharethis-inline-share-buttons"></div>
                <div id="disqus_thread"></div>
                <script>
                    (function() { // DON'T EDIT BELOW THIS LINE
                        var d = document,
                            s = d.createElement('script');
                        s.src = '//eapyl-github-io.disqus.com/embed.js';
                        s.setAttribute('data-timestamp', +new Date());
                        (d.head || d.body).appendChild(s);
                    })();
                </script>
            </div>
        </div>
    </div>
    <script async src="..\prism.js"></script><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date(); a = s.createElement(o),
                m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
    
        ga('create', 'UA-87583712-1', 'auto');
        ga('send', 'pageview');
    </script>
    <script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5906f3ca75e4e1001109c19a&product=inline-share-buttons"></script>
</body>

</html>