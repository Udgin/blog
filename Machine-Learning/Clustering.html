<title>Clustering</title><meta name=viewport content="width=device-width, initial-scale=1.0"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.4/css/bulma.min.css><link rel=stylesheet href=..\prism.css><link><style>.content .tag,.content .number{display:inline;padding:inherit;font-size:inherit;line-height:inherit;text-align:inherit;vertical-align:inherit;border-radius:inherit;font-weight:inherit;white-space:inherit;background:inherit;margin:inherit}</style><nav class=navbar role=navigation aria-label="main navigation"><div class=navbar-brand><a class=navbar-item href=..\..\blog/index.html><img src=..\..\blog/favicon.ico width=28 height=28> Programming notes</a> <a role=button class="navbar-burger burger" aria-label=menu aria-expanded=false data-target=navbarBasicExample><span aria-hidden></span> <span aria-hidden></span> <span aria-hidden></span></a></div><div id=navbarBasicExample class=navbar-menu><div class=navbar-start><div class=navbar-item><div class="navbar-item has-dropdown is-hoverable"><a class=navbar-link>About me</a><div class=navbar-dropdown> <a class=navbar-item>Facebook</a> <a class=navbar-item>LinkedIn</a> <a class=navbar-item>GitHub</a> <a class=navbar-item>Email</a></div></div></div></div></div></nav><section class=section><div class=container><div class=content><h1 id=clustering>Clustering</h1><p>Unsupervised learning is contrasted from supervised learning because it uses an unlabeled training set rather than a labeled one.<h3 id=k-means-algorithm>K-Means Algorithm</h3><ol><li>Randomly initialize two points in the dataset called the cluster centroids.<li>Cluster assignment: assign all examples into one of two groups based on which cluster centroid the example is closest to.<li>Move centroid: compute the averages for all the points inside each of the two cluster centroid groups, then move the cluster centroid points to those averages.<li>Re-run (2) and (3) until we have found our clusters.</ol>
<pre><code class=language-none>Randomly initialize K cluster centroids mu(1), mu(2), ..., mu(K)
Repeat:
   for i = 1 to m:
      c(i):= index (from 1 to K) of cluster centroid closest to x(i)
   for k = 1 to K:
      mu(k):= average (mean) of points assigned to cluster k
</code></pre><p>The first for-loop is the 'Cluster Assignment' step. We make a vector c where c(i) represents the centroid assigned to example x(i).<p><span class=math>\[c^{(i)} = argmin_k\ ||x^{(i)} - \mu_k||^2\]</span><p><span class=math>\[||x^{(i)} - \mu_k|| = ||\quad\sqrt{(x_1^i - \mu_{1(k)})^2 + (x_2^i - \mu_{2(k)})^2 + (x_3^i - \mu_{3(k)})^2 + ...}\quad||\]</span><p>The second for-loop is the 'Move Centroid' step where we move each centroid to the average of its group.<p><span class=math>\[\mu_k = \dfrac{1}{n}[x^{(k_1)} + x^{(k_2)} + \dots + x^{(k_n)}] \in \mathbb{R}^n\]</span><p>After a number of iterations the algorithm will converge, where new iterations do not affect the clusters.<h3 id=optimization-objective>Optimization Objective</h3><p>Using these variables we can define our cost function:<p><span class=math>\[J(c^{(i)},\dots,c^{(m)},\mu_1,\dots,\mu_K) = \dfrac{1}{m}\sum_{i=1}^m ||x^{(i)} - \mu_{c^{(i)}}||^2\]</span><p>With k-means, it is not possible for the cost function to sometimes increase. It should always descend.<h3 id=random-initialization>Random Initialization</h3><ol><li>Have K&lt;m. That is, make sure the number of your clusters is less than the number of your training examples.<li>Randomly pick K training examples. (Not mentioned in the lecture, but also be sure the selected examples are unique).<li>Set μ1,…,μK equal to these K examples.</ol><h3 id=choosing-the-number-of-clusters>Choosing the Number of Clusters</h3><p>Choosing K can be quite arbitrary and ambiguous.<p>A way to choose K is to observe how well k-means performs on a downstream purpose. In other words, you choose K that proves to be most useful for some goal you're trying to achieve from using these clusters.<h3 id=dimensionality-reduction>Dimensionality Reduction</h3><p>Motivation I: Data Compression<ol><li>We may want to reduce the dimension of our features if we have a lot of redundant data.<li>To do this, we find two highly correlated features, plot them, and make a new line that seems to describe both features accurately. We place all the new features on this single line.</ol><p>Doing dimensionality reduction will reduce the total data we have to store in computer memory and will speed up our learning algorithm.<p>Motivation II: Visualization<p>It is not easy to visualize data that is more than three dimensions. We can reduce the dimensions of our data to 3 or less in order to plot it.<h3 id=principal-component-analysis-problem-formulation>Principal Component Analysis Problem Formulation</h3><p>The most popular dimensionality reduction algorithm is Principal Component Analysis (PCA)<p>Problem formulation<p>Given two features, x1 and x2, we want to find a single line that effectively describes both features at once. We then map our old features onto this new line to get a new single feature.<p>The goal of PCA is to reduce the average of all the distances of every feature to the projection line. This is the projection error.<p><strong>PCA is not linear regression</strong><ul><li>In linear regression, we are minimizing the squared error from every point to our predictor line. These are vertical distances.<li>In PCA, we are minimizing the shortest distance, or shortest orthogonal distances, to our data points.</ul><h3 id=principal-component-analysis-algorithm>Principal Component Analysis Algorithm</h3><ul><li>Given training set: x(1),x(2),…,x(m)<li>Preprocess (feature scaling/mean normalization):</ul><p><span class=math>\[\mu_j = \dfrac{1}{m}\sum^m_{i=1}x_j^{(i)}\]</span><ul><li>Replace each <span class=math>\(x_j^{(i)}\)</span> with <span class=math>\(x_j^{(i)} - \mu_j\)</span><li>If different features on different scales (e.g., x1 = size of house, x2 = number of bedrooms), scale features to have comparable range of values.</ul><ol><li>Compute "covariance matrix"</ol><p><span class=math>\[Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T\]</span><ol start=2><li>Compute "eigenvectors" of covariance matrix Σ</ol>
<pre><code class=language-none>[U,S,V] = svd(Sigma);
</code></pre><ol start=3><li>Take the first k columns of the U matrix and compute z</ol><p>Summarize:
<pre><code class=language-none>Sigma = (1/m) * X' * X; % compute the covariance matrix
[U,S,V] = svd(Sigma);   % compute our projected directions
Ureduce = U(:,1:k);     % take the first k directions
Z = X * Ureduce;        % compute the projected data points
</code></pre><h3 id=reconstruction-from-compressed-representation>Reconstruction from Compressed Representation</h3><p><span class=math>\[x_{approx}^{(1)} = U_{reduce} \cdot z^{(1)}\]</span><h3 id=choosing-the-number-of-principal-components>Choosing the Number of Principal Components</h3><p>Algorithm for choosing k<ol><li>Try PCA with k=1,2,…<li>Compute <span class=math>\(U_{reduce}, z, x\)</span><li>Check the formula given above that 99% of the variance is retained. If not, go to step one and increase k.</ol><h3 id=advice-for-applying-pca>Advice for Applying PCA</h3><ul><li>Compressions<li>Reduce space of data<li>Speed up algorithm<li>Visualization of data</ul><p><strong>Bad use of PCA</strong>: trying to prevent overfitting.<p>Don't assume you need to do PCA. <strong>Try your full machine learning algorithm without PCA first</strong>. Then use PCA if you find that you need it.<p>More info: <a href=https://www.coursera.org/learn/machine-learning>https://www.coursera.org/learn/machine-learning</a></div><div><div class=sharethis-inline-share-buttons></div></div></div></section><script async src=..\prism.js></script><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>(function(n,t,i,r,u,f,e){n.GoogleAnalyticsObject=u;n[u]=n[u]||function(){(n[u].q=n[u].q||[]).push(arguments)};n[u].l=1*new Date;f=t.createElement(i);e=t.getElementsByTagName(i)[0];f.async=1;f.src=r;e.parentNode.insertBefore(f,e)})(window,document,"script","https://www.google-analytics.com/analytics.js","ga");ga("create","UA-87583712-1","auto");ga("send","pageview")</script><script src="https://platform-api.sharethis.com/js/sharethis.js#property=5906f3ca75e4e1001109c19a&product=inline-share-buttons"></script>