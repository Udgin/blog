<html>

<head>
    <title>Neural-Networks-Learning</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">
    <link rel="stylesheet" href="..\prism.css"></link>
    <style>
        .columns .column {
            padding: .4rem;
        }
        #sidebar-id {
            padding: .4rem;
        }
        code {
            padding: 0;
            overflow-x: scroll;
        }
        img {
            max-width: 100%;
        }
        .off-canvas .off-canvas-sidebar
        {
            background: none;
        }
    </style>
</head>

<body>
    <div class="off-canvas off-canvas-sidebar-show">
        <a class="off-canvas-toggle btn btn-primary btn-action" href="#sidebar-id">
            <i class="icon icon-menu"></i>
        </a>
        <div id="sidebar-id" class="off-canvas-sidebar">
            <ul class="menu">
                <li class="menu-item">
                    <div class="tile tile-centered">
                        <div class="tile-icon"><img class="avatar" src="https://media.licdn.com/dms/image/C4E03AQEtgQbQGiCpUQ/profile-displayphoto-shrink_200_200/0?e=1547683200&v=beta&t=YKT1NVMDvlzVocWQtSP2Y6Z4Eoy-fqPLncAIk45nF2U"
                                alt="Avatar"></div>
                        <div class="tile-content">Yauhen Pyl</div>
                    </div>
                </li>
                <li class="divider" data-content="LINKS">
                </li>
                <li class="menu-item">
                    <a href="https://plus.google.com/+UdginPyl">Google+</a>
                </li>
                <li class="menu-item">
                    <a href="https://www.facebook.com/yauhen.pyl">Facebook</a>
                </li>
                <li class="menu-item">
                    <a href="https://pl.linkedin.com/in/eugenepyl">LinkedIn</a>
                </li>
                <li class="menu-item">
                    <a href="https://github.com/eapyl">GitHub</a>
                </li>
                <li class="menu-item">
                    <a href="mailto:gromkaktus@gmail.com">Email</a>
                </li>
            </ul>
        </div>
        <a class="off-canvas-overlay" href="#close"></a>
        <div class="off-canvas-content">
            <div class="container grid-lg">
                <h1 id="neural-networks-learning">Neural Networks Learning</h1>
<h3 id="cost-function">Cost Function</h3>
<ol type="a">
<li><p>L= total number of layers in the network</p>
</li>
<li><p><span class="math">\(s_l\)</span> = number of units (not counting bias unit) in layer l</p>
</li>
<li><p>K= number of output units/classes</p>
</li>
</ol>
<p><span class="math">\[\begin{gather*}\large J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\end{gather*}\]</span></p>
<h3 id="backpropagation-algorithm">Backpropagation Algorithm</h3>
<p>&quot;Backpropagation&quot; is neural-network terminology for minimizing our cost function, just like what we were doing with gradient descent in logistic and linear regression.</p>
<p>In back propagation we're going to compute for every node:</p>
<p><span class="math">\(\delta_j^{(l)}\)</span> - = &quot;error&quot; of node j in layer l</p>
<p><span class="math">\(a_j^{(l)}\)</span> is activation node j in layer l.</p>
<p>For the last layer, we can compute the vector of delta values with:</p>
<p><span class="math">\[\delta^{(L)} = a^{(L)} - y\]</span></p>
<p>To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left:</p>
<p><span class="math">\[\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ g'(z^{(l)})\]</span></p>
<p><span class="math">\[g'(z^{(l)}) = a^{(l)}\ .*\ (1 - a^{(l)})\]</span></p>
<h3 id="backpropagation-algorithm-1">Backpropagation algorithm</h3>
<p>Given training set <span class="math">\(\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace\)</span>;</p>
<ul>
<li>Set <span class="math">\(\Delta^{(l)}_{i,j}\)</span>= 0 for all (l,i,j)</li>
</ul>
<p>For training example t =1 to m:</p>
<ul>
<li>Set <span class="math">\(a^{(1)} := x^{(t)}\)</span></li>
<li>Perform forward propagation to compute <span class="math">\(a^{(l)}\)</span> for l=2,3,…,L</li>
<li>Using <span class="math">\(y^{(t)}\)</span>, compute <span class="math">\(\delta^{(L)} = a^{(L)} - y^{(t)}\)</span></li>
<li>Compute <span class="math">\(\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}\)</span> using <span class="math">\(\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})\)</span></li>
<li><span class="math">\(\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}\)</span> or with vectorization, <span class="math">\(\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T\)</span></li>
<li><span class="math">\(D^{(l)}_{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}_{i,j} + \lambda\Theta^{(l)}_{i,j}\right)\)</span> If j≠0 NOTE: Typo in lecture slide omits outside parentheses. This version * is correct.</li>
<li><span class="math">\(D^{(l)}_{i,j} := \dfrac{1}{m}\Delta^{(l)}_{i,j}\)</span> If j=0</li>
</ul>
<h3 id="gradient-checking">Gradient Checking</h3>
<p>Gradient checking will assure that our backpropagation works as intended.</p>
<p><span class="math">\[\dfrac{\partial}{\partial\Theta}J(\Theta) \approx \dfrac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}\]</span></p>
<p>Once you've verified once that your backpropagation algorithm is correct, then you don't need to compute gradApprox again. The code to compute gradApprox is very slow.</p>
<h3 id="random-initialization">Random Initialization</h3>
<p>Initializing all theta weights to zero does not work with neural networks. When we backpropagate, all nodes will update to the same value repeatedly.</p>
<h3 id="summary">Summary</h3>
<p>First, pick a network architecture; choose the layout of your neural network, including how many hidden units in each layer and how many layers total.</p>
<ul>
<li>Number of input units = dimension of features <span class="math">\(x^{(i)}\)</span></li>
<li>Number of output units = number of classes</li>
<li>Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units)</li>
<li>Defaults: 1 hidden layer. If more than 1 hidden layer, then the same number of units in every hidden * layer.</li>
</ul>
<p><strong>Training a Neural Network</strong></p>
<ol>
<li>Randomly initialize the weights</li>
<li>Implement forward propagation to get <span class="math">\(h_\theta(x^{(i)})\)</span></li>
<li>Implement the cost function</li>
<li>Implement backpropagation to compute partial derivatives</li>
<li>Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.</li>
<li>Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.</li>
</ol>
<p>When we perform forward and back propagation, we loop on every training example:</p>
<pre><code class="language-none">for i = 1:m,
   Perform forward propagation and backpropagation using example (x(i),y(i))
   (Get activations a(l) and delta terms d(l) for l = 2,...,L
</code></pre>
<p>More info:
<a href="https://www.coursera.org/learn/machine-learning">https://www.coursera.org/learn/machine-learning</a></p>

                <div class="sharethis-inline-share-buttons"></div>
                <div id="disqus_thread"></div>
                <script>
                    (function() { // DON'T EDIT BELOW THIS LINE
                        var d = document,
                            s = d.createElement('script');
                        s.src = '//eapyl-github-io.disqus.com/embed.js';
                        s.setAttribute('data-timestamp', +new Date());
                        (d.head || d.body).appendChild(s);
                    })();
                </script>
            </div>
        </div>
    </div>
    <script async src="..\prism.js"></script><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date(); a = s.createElement(o),
                m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
    
        ga('create', 'UA-87583712-1', 'auto');
        ga('send', 'pageview');
    </script>
    <script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5906f3ca75e4e1001109c19a&product=inline-share-buttons"></script>
</body>

</html>