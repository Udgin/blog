<title>Neural Networks Learning</title><meta name=viewport content="width=device-width, initial-scale=1.0"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.4/css/bulma.min.css><link rel=stylesheet href=..\prism.css><link><style>.content .tag,.content .number{display:inline;padding:inherit;font-size:inherit;line-height:inherit;text-align:inherit;vertical-align:inherit;border-radius:inherit;font-weight:inherit;white-space:inherit;background:inherit;margin:inherit}</style><nav class=navbar role=navigation aria-label="main navigation"><div class=navbar-brand><a class=navbar-item href=..\index.html><img src=..\favicon.ico width=28 height=28> Programming notes</a> <a role=button class="navbar-burger burger" aria-label=menu aria-expanded=false data-target=navbarBasicExample><span aria-hidden></span> <span aria-hidden></span> <span aria-hidden></span></a></div><div id=navbarBasicExample class=navbar-menu><div class=navbar-start><div class=navbar-item><div class="navbar-item has-dropdown is-hoverable"><a class=navbar-link>About me</a><div class=navbar-dropdown> <a class=navbar-item>Facebook</a> <a class=navbar-item>LinkedIn</a> <a class=navbar-item>GitHub</a> <a class=navbar-item>Email</a></div></div></div></div></div></nav><section class=section><div class=container><div class=content><h1 id=neural-networks-learning>Neural Networks Learning</h1><h3 id=cost-function>Cost Function</h3><ol type=a><li><p>L= total number of layers in the network<li><p><span class=math>\(s_l\)</span> = number of units (not counting bias unit) in layer l<li><p>K= number of output units/classes</ol><p><span class=math>\[\begin{gather*}\large J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\end{gather*}\]</span><h3 id=backpropagation-algorithm>Backpropagation Algorithm</h3><p>"Backpropagation" is neural-network terminology for minimizing our cost function, just like what we were doing with gradient descent in logistic and linear regression.<p>In back propagation we're going to compute for every node:<p><span class=math>\(\delta_j^{(l)}\)</span> - = "error" of node j in layer l<p><span class=math>\(a_j^{(l)}\)</span> is activation node j in layer l.<p>For the last layer, we can compute the vector of delta values with:<p><span class=math>\[\delta^{(L)} = a^{(L)} - y\]</span><p>To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left:<p><span class=math>\[\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ g'(z^{(l)})\]</span><p><span class=math>\[g'(z^{(l)}) = a^{(l)}\ .*\ (1 - a^{(l)})\]</span><h3 id=backpropagation-algorithm-1>Backpropagation algorithm</h3><p>Given training set <span class=math>\(\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace\)</span>;<ul><li>Set <span class=math>\(\Delta^{(l)}_{i,j}\)</span>= 0 for all (l,i,j)</ul><p>For training example t =1 to m:<ul><li>Set <span class=math>\(a^{(1)} := x^{(t)}\)</span><li>Perform forward propagation to compute <span class=math>\(a^{(l)}\)</span> for l=2,3,…,L<li>Using <span class=math>\(y^{(t)}\)</span>, compute <span class=math>\(\delta^{(L)} = a^{(L)} - y^{(t)}\)</span><li>Compute <span class=math>\(\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}\)</span> using <span class=math>\(\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})\)</span><li><span class=math>\(\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}\)</span> or with vectorization, <span class=math>\(\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T\)</span><li><span class=math>\(D^{(l)}_{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}_{i,j} + \lambda\Theta^{(l)}_{i,j}\right)\)</span> If j≠0 NOTE: Typo in lecture slide omits outside parentheses. This version * is correct.<li><span class=math>\(D^{(l)}_{i,j} := \dfrac{1}{m}\Delta^{(l)}_{i,j}\)</span> If j=0</ul><h3 id=gradient-checking>Gradient Checking</h3><p>Gradient checking will assure that our backpropagation works as intended.<p><span class=math>\[\dfrac{\partial}{\partial\Theta}J(\Theta) \approx \dfrac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}\]</span><p>Once you've verified once that your backpropagation algorithm is correct, then you don't need to compute gradApprox again. The code to compute gradApprox is very slow.<h3 id=random-initialization>Random Initialization</h3><p>Initializing all theta weights to zero does not work with neural networks. When we backpropagate, all nodes will update to the same value repeatedly.<h3 id=summary>Summary</h3><p>First, pick a network architecture; choose the layout of your neural network, including how many hidden units in each layer and how many layers total.<ul><li>Number of input units = dimension of features <span class=math>\(x^{(i)}\)</span><li>Number of output units = number of classes<li>Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units)<li>Defaults: 1 hidden layer. If more than 1 hidden layer, then the same number of units in every hidden * layer.</ul><p><strong>Training a Neural Network</strong><ol><li>Randomly initialize the weights<li>Implement forward propagation to get <span class=math>\(h_\theta(x^{(i)})\)</span><li>Implement the cost function<li>Implement backpropagation to compute partial derivatives<li>Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.<li>Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.</ol><p>When we perform forward and back propagation, we loop on every training example:
<pre><code class=language-none>for i = 1:m,
   Perform forward propagation and backpropagation using example (x(i),y(i))
   (Get activations a(l) and delta terms d(l) for l = 2,...,L
</code></pre><p>More info: <a href=https://www.coursera.org/learn/machine-learning>https://www.coursera.org/learn/machine-learning</a></div><div><div class=sharethis-inline-share-buttons></div></div></div></section><script async src=..\prism.js></script><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>(function(n,t,i,r,u,f,e){n.GoogleAnalyticsObject=u;n[u]=n[u]||function(){(n[u].q=n[u].q||[]).push(arguments)};n[u].l=1*new Date;f=t.createElement(i);e=t.getElementsByTagName(i)[0];f.async=1;f.src=r;e.parentNode.insertBefore(f,e)})(window,document,"script","https://www.google-analytics.com/analytics.js","ga");ga("create","UA-87583712-1","auto");ga("send","pageview")</script><script src="https://platform-api.sharethis.com/js/sharethis.js#property=5906f3ca75e4e1001109c19a&product=inline-share-buttons"></script>