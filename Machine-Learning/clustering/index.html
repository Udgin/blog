<!DOCTYPE html>
<html lang="en">
    <head>
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <meta http-equiv="content-type" content="text/html; charset=utf-8">

      <!-- Enable responsiveness on mobile devices-->
      <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

      <title>Notes - Clustering</title>

      

      
          <script src="https://cdnjs.cloudflare.com/ajax/libs/slideout/1.0.1/slideout.min.js"></script>
          
      

      
          <link rel="stylesheet" href="https:&#x2F;&#x2F;eapyl.github.io&#x2F;site.css">
          
      

      
      
    </head>

    <body>
        <div class="container">

            <div id="mobile-navbar" class="mobile-navbar">
              <div class="mobile-header-logo">
                <a href="/" class="logo">Notes</a>
              </div>
              <div class="mobile-navbar-icon icon-out">
                <span></span>
                <span></span>
                <span></span>
              </div>
            </div>

            <nav id="mobile-menu" class="mobile-menu slideout-menu slideout-menu-left">
              <ul class="mobile-menu-list">
                
                    <li class="mobile-menu-item">
                        <a href="https:&#x2F;&#x2F;eapyl.github.io&#x2F;">
                            Home
                        </a>
                    </li>
                
                    <li class="mobile-menu-item">
                        <a href="https:&#x2F;&#x2F;eapyl.github.io&#x2F;&#x2F;categories">
                            Categories
                        </a>
                    </li>
                
                    <li class="mobile-menu-item">
                        <a href="https:&#x2F;&#x2F;eapyl.github.io&#x2F;&#x2F;tags">
                            Tags
                        </a>
                    </li>
                
                    <li class="mobile-menu-item">
                        <a href="https:&#x2F;&#x2F;eapyl.github.io&#x2F;&#x2F;about">
                            About
                        </a>
                    </li>
                
              </ul>
            </nav>

            <header id="header">
                <div class="logo"><a href="https:&#x2F;&#x2F;eapyl.github.io&#x2F;">Notes</a></div>
                <nav class="menu">
                    <ul>
                        
                            <li>
                                <a href="https:&#x2F;&#x2F;eapyl.github.io&#x2F;">
                                    Home
                                </a>
                            </li>
                        
                            <li>
                                <a href="https:&#x2F;&#x2F;eapyl.github.io&#x2F;&#x2F;categories">
                                    Categories
                                </a>
                            </li>
                        
                            <li>
                                <a href="https:&#x2F;&#x2F;eapyl.github.io&#x2F;&#x2F;tags">
                                    Tags
                                </a>
                            </li>
                        
                            <li>
                                <a href="https:&#x2F;&#x2F;eapyl.github.io&#x2F;&#x2F;about">
                                    About
                                </a>
                            </li>
                        
                    </ul>
                </nav>
            </header>

            <main>
                <div class="content" id="mobile-panel">
                    


<div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">Contents</h2>
    <div class="post-toc-content always-active">
        <nav id="TableOfContents">
            <ul>
                
                <li>
                    <a href="https://eapyl.github.io/Machine-Learning/clustering/#k-means-algorithm" class="toc-link">K-Means Algorithm</a>
                    
                </li>
                
                <li>
                    <a href="https://eapyl.github.io/Machine-Learning/clustering/#optimization-objective" class="toc-link">Optimization Objective</a>
                    
                </li>
                
                <li>
                    <a href="https://eapyl.github.io/Machine-Learning/clustering/#random-initialization" class="toc-link">Random Initialization</a>
                    
                </li>
                
                <li>
                    <a href="https://eapyl.github.io/Machine-Learning/clustering/#choosing-the-number-of-clusters" class="toc-link">Choosing the Number of Clusters</a>
                    
                </li>
                
                <li>
                    <a href="https://eapyl.github.io/Machine-Learning/clustering/#dimensionality-reduction" class="toc-link">Dimensionality Reduction</a>
                    
                </li>
                
                <li>
                    <a href="https://eapyl.github.io/Machine-Learning/clustering/#principal-component-analysis-problem-formulation" class="toc-link">Principal Component Analysis Problem Formulation</a>
                    
                </li>
                
                <li>
                    <a href="https://eapyl.github.io/Machine-Learning/clustering/#principal-component-analysis-algorithm" class="toc-link">Principal Component Analysis Algorithm</a>
                    
                </li>
                
                <li>
                    <a href="https://eapyl.github.io/Machine-Learning/clustering/#reconstruction-from-compressed-representation" class="toc-link">Reconstruction from Compressed Representation</a>
                    
                </li>
                
                <li>
                    <a href="https://eapyl.github.io/Machine-Learning/clustering/#choosing-the-number-of-principal-components" class="toc-link">Choosing the Number of Principal Components</a>
                    
                </li>
                
                <li>
                    <a href="https://eapyl.github.io/Machine-Learning/clustering/#advice-for-applying-pca" class="toc-link">Advice for Applying PCA</a>
                    
                </li>
                
            </ul>
        </nav>
    </div>
</div>


<article class="post">
    
    <header class="post__header">
        <h1 class="post__title">
            <a href="https:&#x2F;&#x2F;eapyl.github.io&#x2F;Machine-Learning&#x2F;clustering&#x2F;">Clustering</a>
        </h1>
        <div class="post__meta">
            <span class="post__time">2016-09-27</span>
            
        </div>
    </header>

    <div class="post-content">
      <p>Unsupervised learning is contrasted from supervised learning because it uses an unlabeled training set rather than a labeled one.</p>
<p id="zola-continue-reading"><a name="continue-reading"></a></p>
<h3 id="k-means-algorithm">K-Means Algorithm</h3>
<ol>
<li>Randomly initialize two points in the dataset called the cluster centroids.</li>
<li>Cluster assignment: assign all examples into one of two groups based on which cluster centroid the example is closest to.</li>
<li>Move centroid: compute the averages for all the points inside each of the two cluster centroid groups, then move the cluster centroid points to those averages.</li>
<li>Re-run (2) and (3) until we have found our clusters.</li>
</ol>
<pre style="background-color:#2b303b;">
<span style="color:#c0c5ce;">Randomly initialize K cluster centroids mu(1), mu(2), ..., mu(K)
Repeat:
   for i = 1 to m:
      c(i):= index (from 1 to K) of cluster centroid closest to x(i)
   for k = 1 to K:
      mu(k):= average (mean) of points assigned to cluster k
</span></pre>
<p>The first for-loop is the 'Cluster Assignment' step. We make a vector c where c(i) represents the centroid assigned to example x(i).</p>
<p>$$[c^{(i)} = argmin_k\ ||x^{(i)} - \mu_k||^2]$$</p>
<p>$$[||x^{(i)} - \mu_k|| = ||\quad\sqrt{(x_1^i - \mu_{1(k)})^2 + (x_2^i - \mu_{2(k)})^2 + (x_3^i - \mu_{3(k)})^2 + ...}\quad||]$$</p>
<p>The second for-loop is the 'Move Centroid' step where we move each centroid to the average of its group.</p>
<p>$$[\mu_k = \dfrac{1}{n}[x^{(k_1)} + x^{(k_2)} + \dots + x^{(k_n)}] \in \mathbb{R}^n]$$</p>
<p>After a number of iterations the algorithm will converge, where new iterations do not affect the clusters.</p>
<h3 id="optimization-objective">Optimization Objective</h3>
<p>Using these variables we can define our cost function:</p>
<p>$$[J(c^{(i)},\dots,c^{(m)},\mu_1,\dots,\mu_K) = \dfrac{1}{m}\sum_{i=1}^m ||x^{(i)} - \mu_{c^{(i)}}||^2]$$</p>
<p>With k-means, it is not possible for the cost function to sometimes increase. It should always descend.</p>
<h3 id="random-initialization">Random Initialization</h3>
<ol>
<li>Have K&lt;m. That is, make sure the number of your clusters is less than the number of your training examples.</li>
<li>Randomly pick K training examples. (Not mentioned in the lecture, but also be sure the selected examples are unique).</li>
<li>Set μ1,…,μK equal to these K examples.</li>
</ol>
<h3 id="choosing-the-number-of-clusters">Choosing the Number of Clusters</h3>
<p>Choosing K can be quite arbitrary and ambiguous.</p>
<p>A way to choose K is to observe how well k-means performs on a downstream purpose. In other words, you choose K that proves to be most useful for some goal you're trying to achieve from using these clusters.</p>
<h3 id="dimensionality-reduction">Dimensionality Reduction</h3>
<p>Motivation I: Data Compression</p>
<ol>
<li>We may want to reduce the dimension of our features if we have a lot of redundant data.</li>
<li>To do this, we find two highly correlated features, plot them, and make a new line that seems to describe both features accurately. We place all the new features on this single line.</li>
</ol>
<p>Doing dimensionality reduction will reduce the total data we have to store in computer memory and will speed up our learning algorithm.</p>
<p>Motivation II: Visualization</p>
<p>It is not easy to visualize data that is more than three dimensions. We can reduce the dimensions of our data to 3 or less in order to plot it.</p>
<h3 id="principal-component-analysis-problem-formulation">Principal Component Analysis Problem Formulation</h3>
<p>The most popular dimensionality reduction algorithm is Principal Component Analysis (PCA)</p>
<p>Problem formulation</p>
<p>Given two features, x1 and x2, we want to find a single line that effectively describes both features at once. We then map our old features onto this new line to get a new single feature.</p>
<p>The goal of PCA is to reduce the average of all the distances of every feature to the projection line. This is the projection error.</p>
<p><strong>PCA is not linear regression</strong></p>
<ul>
<li>In linear regression, we are minimizing the squared error from every point to our predictor line. These are vertical distances.</li>
<li>In PCA, we are minimizing the shortest distance, or shortest orthogonal distances, to our data points.</li>
</ul>
<h3 id="principal-component-analysis-algorithm">Principal Component Analysis Algorithm</h3>
<ul>
<li>Given training set: x(1),x(2),…,x(m)</li>
<li>Preprocess (feature scaling/mean normalization):</li>
</ul>
<p>$$[\mu_j = \dfrac{1}{m}\sum^m_{i=1}x_j^{(i)}]$$</p>
<ul>
<li>Replace each $$(x_j^{(i)})$$ with $$(x_j^{(i)} - \mu_j)$$</li>
<li>If different features on different scales (e.g., x1 = size of house, x2 = number of bedrooms), scale features to have comparable range of values.</li>
</ul>
<ol>
<li>Compute &quot;covariance matrix&quot;</li>
</ol>
<p>$$[Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T]$$</p>
<ol start="2">
<li>Compute &quot;eigenvectors&quot; of covariance matrix Σ</li>
</ol>
<pre style="background-color:#2b303b;">
<span style="color:#c0c5ce;">[U,S,V] = svd(Sigma);
</span></pre>
<ol start="3">
<li>Take the first k columns of the U matrix and compute z</li>
</ol>
<p>Summarize:</p>
<pre style="background-color:#2b303b;">
<span style="color:#c0c5ce;">Sigma = (1/m) * X&#39; * X; % compute the covariance matrix
[U,S,V] = svd(Sigma);   % compute our projected directions
Ureduce = U(:,1:k);     % take the first k directions
Z = X * Ureduce;        % compute the projected data points
</span></pre><h3 id="reconstruction-from-compressed-representation">Reconstruction from Compressed Representation</h3>
<p>$$[x_{approx}^{(1)} = U_{reduce} \cdot z^{(1)}]$$</p>
<h3 id="choosing-the-number-of-principal-components">Choosing the Number of Principal Components</h3>
<p>Algorithm for choosing k</p>
<ol>
<li>Try PCA with k=1,2,…</li>
<li>Compute $$(U_{reduce}, z, x)$$</li>
<li>Check the formula given above that 99% of the variance is retained. If not, go to step one and increase k.</li>
</ol>
<h3 id="advice-for-applying-pca">Advice for Applying PCA</h3>
<ul>
<li>Compressions</li>
<li>Reduce space of data</li>
<li>Speed up algorithm</li>
<li>Visualization of data</li>
</ul>
<p><strong>Bad use of PCA</strong>: trying to prevent overfitting. </p>
<p>Don't assume you need to do PCA. <strong>Try your full machine learning algorithm without PCA first</strong>. Then use PCA if you find that you need it.</p>
<p>More info:
<a href="https://www.coursera.org/learn/machine-learning">https://www.coursera.org/learn/machine-learning</a></p>

    </div>

    
    

    <div class="post-footer">
        
            
                <div class="post-tags">
                    
                        <a href="https:&#x2F;&#x2F;eapyl.github.io&#x2F;tags&#x2F;machine-learning&#x2F;">#machine learning</a>
                    
                </div>
            
            

        

    </div>

    
    
</article>


                </div>
            </main>

            
            
        </div>

      
          <script type="text/javascript" src="https:&#x2F;&#x2F;eapyl.github.io&#x2F;even.js" ></script>
      
    </body>

</html>
