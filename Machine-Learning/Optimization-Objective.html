<title>Optimization Objective</title><meta name=viewport content="width=device-width, initial-scale=1.0"><link rel=stylesheet href=https://unpkg.com/spectre.css/dist/spectre.min.css><link rel=stylesheet href=https://unpkg.com/spectre.css/dist/spectre-exp.min.css><link rel=stylesheet href=https://unpkg.com/spectre.css/dist/spectre-icons.min.css><style>.columns .column{padding:.4rem}#sidebar-id{padding:.4rem}code{padding:0;overflow-x:scroll}img{max-width:100%}.off-canvas .off-canvas-sidebar{background:none}.off-canvas .off-canvas-content{padding:0}.panel-body>h1:first-child{padding-left:50px}body{background-color:#f7f8f9}.w-back{background-color:#fff}.panel{margin-top:.4rem}table>thead>tr,table>tbody>tr:nth-child(odd){background-color:#f7f8f9}</style><div class="off-canvas off-canvas-sidebar-show back"><a class="off-canvas-toggle btn btn-primary btn-action" href=#sidebar-id><i class="icon icon-menu"></i></a><div id=sidebar-id class=off-canvas-sidebar><ul class=menu><li class=menu-item><div class="tile tile-centered"><div class=tile-icon><img class=avatar src="https://media.licdn.com/dms/image/C4E03AQEtgQbQGiCpUQ/profile-displayphoto-shrink_200_200/0?e=1554336000&v=beta&t=6D8qRHwmkRqNspcpPueQ_ADqtZUqISRaPveGI7FT6fo" alt=Avatar></div><div class=tile-content>Yauhen Pyl</div></div><li class=divider data-content=LINKS><li class=menu-item><a href=https://plus.google.com/+UdginPyl>Google+</a><li class=menu-item><a href=https://www.facebook.com/yauhen.pyl>Facebook</a><li class=menu-item><a href=https://pl.linkedin.com/in/yauhenpyl>LinkedIn</a><li class=menu-item><a href=https://github.com/eapyl>GitHub</a><li class=menu-item><a href=mailto:gromkaktus@gmail.com>Email</a></ul></div><a class=off-canvas-overlay href=#close></a><div class=off-canvas-content><div class="container grid-lg"><div class="panel w-back"><div class=panel-body><h1 id=optimization-objective>Optimization Objective</h1><p>The Support Vector Machine (SVM) is yet another type of supervised machine learning algorithm. It is sometimes cleaner and more powerful.<h3 id=cost-function>Cost function</h3><p><span class=math>\(\text{cost}_1(z)\)</span> and <span class=math>\(\text{cost}_0(z)\)</span> (respectively, note that <span class=math>\(\text{cost}_1(z)\)</span> is the cost for classifying when y=1, and <span class=math>\(\text{cost}_0(z)\)</span> is the cost for classifying when y=0)<p><img src=Optimization-Objective\Svm_hing.png alt=image> <img src=Optimization-Objective\Svm_hinge_negative_class.png alt=image><p><span class=math>\[J(\theta) = C\sum_{i=1}^m y^{(i)} \ \text{cost}_1(\theta^Tx^{(i)}) + (1 - y^{(i)}) \ \text{cost}_0(\theta^Tx^{(i)}) + \dfrac{1}{2}\sum_{j=1}^n \Theta^2_j\]</span><p>Note that the hypothesis of the Support Vector Machine is not interpreted as the probability of y being 1 or 0 (as it is for the hypothesis of logistic regression). Instead, it outputs either 1 or 0. (In technical terms, it is a discriminant function.)<p><span class=math>\[h_\theta(x) =\begin{cases} 1 &amp; \text{if} \ \Theta^Tx \geq 0 \\ 0 &amp; \text{otherwise}\end{cases}\]</span><h3 id=large-margin-intuition>Large Margin Intuition</h3><p>A useful way to think about Support Vector Machines is to think of them as Large Margin Classifiers.<ul><li>If y=1, we want <span class=math>\(\Theta^Tx \geq 1\)</span> (not just ≥0)<li>If y=0, we want <span class=math>\(\Theta^Tx \leq -1\)</span> (not just &lt;0)</ul><p>In SVMs, the decision boundary has the special property that it is as far away as possible from both the positive and the negative examples.<p>This large margin is only achieved when C is very large. Data is linearly separable when a straight line can separate the positive and negative examples. If we have outlier examples that we don't want to affect the decision boundary, then we can reduce C. Increasing and decreasing C is similar to respectively decreasing and increasing λ, and can simplify our decision boundary.<h3 id=kernels>Kernels</h3><p>Kernels allow us to make complex, non-linear classifiers using Support Vector Machines.<p><span class=math>\[f_i = similarity(x, l^{(i)}) = \exp(-\dfrac{||x - l^{(i)}||^2}{2\sigma^2})\]</span><p>This "similarity" function is called a Gaussian Kernel. It is a specific example of a kernel.<p>If <span class=math>\(x \approx l^{(i)}\)</span>, then <span class=math>\(f_i = \exp(-\dfrac{\approx 0^2}{2\sigma^2}) \approx 1\)</span> If x is far from <span class=math>\(l^{(i)}\)</span>, then <span class=math>\(f_i = \exp(-\dfrac{(large\ number)^2}{2\sigma^2}) \approx 0\)</span><p>One way to get the landmarks is to put them in the exact same locations as all the training examples. This gives us m landmarks, with one landmark per training example.<p>Using kernels to generate f(i) is not exclusive to SVMs and may also be applied to logistic regression. However, because of computational optimizations on SVMs, kernels combined with SVMs is much faster than with other algorithms, so kernels are almost always found combined only with SVMs.<h3 id=choosing-svm-parameters>Choosing SVM Parameters</h3><ul><li><p>If C is large, then we get higher variance/lower bias<li><p>If C is small, then we get lower variance/higher bias<li><p>With a large <span class=math>\(σ^2\)</span>, the features fi vary more smoothly, causing higher bias and lower variance.<li><p>With a small <span class=math>\(σ^2\)</span>, the features fi vary less smoothly, causing lower bias and higher variance.</ul><h3 id=using-an-svm>Using An SVM</h3><ul><li>Choice of parameter C<li>Choice of kernel (similarity function)<li>No kernel ("linear" kernel) -- gives standard linear classifier<li>Choose when n is large and when m is small<li>Gaussian Kernel (above) -- need to choose σ2<li>Choose when n is small and m is large</ul><h3 id=logistic-regression-vs.svms>Logistic Regression vs. SVMs</h3><ul><li>If n is large (relative to m), then use logistic regression, or SVM without a kernel (the "linear kernel")<li>If n is small and m is intermediate, then use SVM with a Gaussian Kernel<li>If n is small and m is large, then manually create/add more features, then use logistic regression or SVM without a kernel. In the first case, we don't have enough examples to need a complicated polynomial hypothesis. In the second example, we have enough examples that we may need a complex non-linear hypothesis. In the last case, we want to increase our features so that logistic regression becomes applicable.</ul><p><strong>Note</strong>: a neural network is likely to work well for any of these situations, but may be slower to train.<p>More info: <a href=https://www.coursera.org/learn/machine-learning>https://www.coursera.org/learn/machine-learning</a></div><div class=panel-footer><div class=sharethis-inline-share-buttons></div><div id=disqus_thread></div><script>(function(){var n=document,t=n.createElement("script");t.src="//eapyl-github-io.disqus.com/embed.js";t.setAttribute("data-timestamp",+new Date);(n.head||n.body).appendChild(t)})()</script></div></div></div></div></div><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>(function(n,t,i,r,u,f,e){n.GoogleAnalyticsObject=u;n[u]=n[u]||function(){(n[u].q=n[u].q||[]).push(arguments)};n[u].l=1*new Date;f=t.createElement(i);e=t.getElementsByTagName(i)[0];f.async=1;f.src=r;e.parentNode.insertBefore(f,e)})(window,document,"script","https://www.google-analytics.com/analytics.js","ga");ga("create","UA-87583712-1","auto");ga("send","pageview")</script><script src="https://platform-api.sharethis.com/js/sharethis.js#property=5906f3ca75e4e1001109c19a&product=inline-share-buttons"></script>